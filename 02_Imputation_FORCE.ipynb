{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORCE 2020Well Log Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuXerCODbXmG"
   },
   "source": [
    "Firstly we try preparing the data and importing the liabraries important for the problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADZhgpaOblJ1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project has been sourced from the FORCE 2020 Facies prediction competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', sep=';')\n",
    "test = pd.read_csv('data/test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FORMATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(pd.concat([train, test]).iloc[:, :-5])\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "rect = fig.patch\n",
    "rect.set_facecolor('white')\n",
    "plt.savefig('figures/force_missingno.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_encoder = LabelEncoder()\n",
    "group_encoder = LabelEncoder()\n",
    "form_encoder = LabelEncoder()\n",
    "\n",
    "well_encoder.fit(pd.concat([train, test]).WELL.replace(np.nan, ''))\n",
    "group_encoder.fit(pd.concat([train, test]).GROUP.replace(np.nan, ''))\n",
    "form_encoder.fit(pd.concat([train, test]).FORMATION.replace(np.nan, ''))\n",
    "\n",
    "train['WELLe'] = well_encoder.transform(train.WELL.replace(np.nan, ''))\n",
    "train['WELLe'] = train['WELLe'].astype(int)\n",
    "train['GROUPe'] = group_encoder.transform(train.GROUP.replace(np.nan, ''))\n",
    "train['GROUPe'] = train['GROUPe'].astype(int)\n",
    "train['FORMATIONe'] = form_encoder.transform(train.FORMATION.replace(np.nan, ''))\n",
    "train['FORMATIONe'] = train['FORMATIONe'].astype(int)\n",
    "\n",
    "test['WELLe'] = well_encoder.transform(test.WELL.replace(np.nan, ''))\n",
    "test['WELLe'] = test['WELLe'].astype(int)\n",
    "test['GROUPe'] = group_encoder.transform(test.GROUP.replace(np.nan, ''))\n",
    "test['GROUPe'] = test['GROUPe'].astype(int)\n",
    "test['FORMATIONe'] = form_encoder.transform(test.FORMATION.replace(np.nan, ''))\n",
    "test['FORMATIONe'] = test['FORMATIONe'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in test.columns:\n",
    "    print(column, np.sum(train[column].isna())/3693222*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a map and see where the training wells and test wells are located in cartesian space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "y5a-x-zKf9XS",
    "outputId": "0e7f0a1f-d7e5-4982-dd45-469bf165cbc5"
   },
   "outputs": [],
   "source": [
    "head = train.sort_values('DEPTH_MD').drop_duplicates(['WELL'])\n",
    "head_test = test.sort_values('DEPTH_MD').drop_duplicates(['WELL'])\n",
    "fig = plt.figure(figsize=(4, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.grid()\n",
    "ax.scatter(head.X_LOC, head.Y_LOC, label='train')\n",
    "ax.scatter(head_test.X_LOC, head_test.Y_LOC, label='test')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "ax.set_xlabel('East (m)')\n",
    "ax.set_ylabel('North (m)')\n",
    "fig.savefig('figures/f2020_location_map.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Statistics\n",
    "The number of missing values and missing values per well and per zone.\n",
    "\n",
    "Wells where key logs DTE, DTSE and RHOBE are missing? Are there any trends here, how might the distribution of missing data affect our imputation performance? **In your accuracy score Debajoy it would be useful to do some deeper analysis to see if any zones perform better than others, an important part of this process is to understand the limitations of the algorithm you are using and to know when and where it will and won't work -> and for a paper, most importantly, why.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])\n",
    "missing_pc = pd.DataFrame({'ALL':data.count()/data.shape[0]})\n",
    "for well, sub in data.groupby('WELL'):\n",
    "    missing_pc[well] = 1- sub.count()/sub.shape[0]\n",
    "for zn, sub in data.groupby('FORMATION'):\n",
    "    missing_pc[zn] = 1- sub.count()/sub.shape[0]\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "plasma_r5 = mpl.cm.get_cmap('plasma_r', 11)\n",
    "plasma_r5.colors[0] = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(missing_pc.T.iloc[:110, :-5], cbar_kws={'label':'Fraction Missing'}, cmap=plasma_r5, vmin=-0.1, vmax=1)\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/well_fraction_f20_missing.png', dpi=150)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(missing_pc.T.iloc[110:, :-5], cbar_kws={'label':'Fraction Missing'}, cmap=plasma_r5, vmin=-0.1, vmax=1)\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/zone_fraction_f20_missing.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgU_iNyDp7HP"
   },
   "source": [
    "## Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "DnG_YSgWm-Mz",
    "outputId": "41216a60-c26b-4061-91ff-9f179c91a32d"
   },
   "outputs": [],
   "source": [
    "# logs chosen as input for this set of model\n",
    "training_logs = ['X_LOC', 'Y_LOC', 'Z_LOC',\n",
    "       'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF',\n",
    "       'DTC', 'SP', 'ROP', 'DTS', 'DRHO', 'WELLe', 'GROUPe', 'FORMATIONe']\n",
    "\n",
    "# dropping rows that are all NA (i.e. 10 of the logs are all nan)\n",
    "train_fe = train[training_logs].dropna(thresh=13).copy()\n",
    "print(train_fe.shape)\n",
    "\n",
    "train_fe['RD10'] = np.log10(train_fe['RDEP']+1)\n",
    "train_fe['RM10'] = np.log10(train_fe['RMED']+1)\n",
    "train_fe['RS10'] = np.log10(train_fe['RSHA']+1)\n",
    "\n",
    "# repeat for test data\n",
    "test_fe = test[training_logs].dropna(thresh=13).copy()\n",
    "print(test_fe.shape)\n",
    "\n",
    "test_fe['RD10'] = np.log10(test_fe['RDEP']+1)\n",
    "test_fe['RM10'] = np.log10(test_fe['RMED']+1)\n",
    "test_fe['RS10'] = np.log10(test_fe['RSHA']+1)\n",
    "\n",
    "feature_logs = ['X_LOC', 'Y_LOC', 'Z_LOC',\n",
    "       'CALI', 'RS10', 'RM10', 'RD10', 'RHOB', 'GR', 'NPHI', 'PEF',\n",
    "       'DTC', 'SP', 'ROP', 'DTS', 'DRHO', 'WELLe', 'GROUPe', 'FORMATIONe']\n",
    "\n",
    "# scaling\n",
    "sscaler = StandardScaler()\n",
    "sscaler.fit(train_fe)\n",
    "\n",
    "train_fe.loc[:, :] = sscaler.transform(train_fe)\n",
    "test_fe.loc[:, :] = sscaler.transform(test_fe)\n",
    "\n",
    "# lets add a feature for plotting that tells us how many sample of a super-group is available for training\n",
    "pc_missing_zone_dtc = dict()\n",
    "pc_missing_zone_dts = dict()\n",
    "pc_missing_zone_rhob = dict()\n",
    "\n",
    "for fm, val in train_fe.groupby(\"GROUPe\"):\n",
    "    if val.shape[0] == 0:\n",
    "        a = 1\n",
    "        b = 1\n",
    "        c = 1\n",
    "    else:\n",
    "        a = val.shape[0] - np.sum(val.DTC.isna())\n",
    "        b = val.shape[0] - np.sum(val.DTS.isna())\n",
    "        c = val.shape[0] - np.sum(val.RHOB.isna())\n",
    "    pc_missing_zone_dtc[f\"{fm:.08f}\"] = a\n",
    "    pc_missing_zone_dts[f\"{fm:.08f}\"] = b\n",
    "    pc_missing_zone_rhob[f\"{fm:.08f}\"] = c\n",
    "    \n",
    "test_fe[\"Grp_TSS_DTC\"] = test_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dtc)\n",
    "test_fe[\"Grp_TSS_DTS\"] = test_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dts)\n",
    "test_fe[\"Grp_TSS_RHOB\"] = test_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_rhob)\n",
    "train_fe[\"Grp_TSS_DTC\"] = train_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dtc)\n",
    "train_fe[\"Grp_TSS_DTS\"] = train_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dts)\n",
    "train_fe[\"Grp_TSS_RHOB\"] = train_fe.GROUPe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_rhob)\n",
    "\n",
    "# lets add a feature for plotting that tells us how many samples of a formation is available for training\n",
    "pc_missing_zone_dtc = dict()\n",
    "pc_missing_zone_dts = dict()\n",
    "pc_missing_zone_rhob = dict()\n",
    "\n",
    "for fm, val in train_fe.groupby(\"FORMATIONe\"):\n",
    "    if val.shape[0] == 0:\n",
    "        a = 1\n",
    "        b = 1\n",
    "        c = 1\n",
    "    else:\n",
    "        a = val.shape[0] - np.sum(val.DTC.isna())\n",
    "        b = val.shape[0] - np.sum(val.DTS.isna())\n",
    "        c = val.shape[0] - np.sum(val.RHOB.isna())\n",
    "    pc_missing_zone_dtc[f\"{fm:.08f}\"] = a\n",
    "    pc_missing_zone_dts[f\"{fm:.08f}\"] = b\n",
    "    pc_missing_zone_rhob[f\"{fm:.08f}\"] = c\n",
    "    \n",
    "# TSS -> Training Sample Size\n",
    "test_fe[\"Fm_TSS_DTC\"] = test_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dtc)\n",
    "test_fe[\"Fm_TSS_DTS\"] = test_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dts)\n",
    "test_fe[\"Fm_TSS_RHOB\"] = test_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_rhob)\n",
    "train_fe[\"Fm_TSS_DTC\"] = train_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dtc)\n",
    "train_fe[\"Fm_TSS_DTS\"] = train_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_dts)\n",
    "train_fe[\"Fm_TSS_RHOB\"] = train_fe.FORMATIONe.apply(lambda x: f\"{x:.08f}\").map(pc_missing_zone_rhob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data, j, set_to_nan=0.3):\n",
    "    \"\"\"This method sets set_to_nan fraction of the values to nan so we can measure the model accuracy.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    sub = data.dropna(subset=[j])\n",
    "    rand_set_mask = np.random.random(len(sub)) < set_to_nan\n",
    "    replace = sub.index[rand_set_mask]\n",
    "    data.loc[replace, j] = np.nan\n",
    "    data['set_nan'] = False\n",
    "    data.loc[replace, 'set_nan'] = True\n",
    "    data['was_nan'] = data[j].isna()\n",
    "    print('Col, InputSize, Number of Nan, % NaN, Original Nan', 'Training Size')\n",
    "    print(\n",
    "        f'{j:>3}',\n",
    "        f'{data.shape[0]:>10}',\n",
    "        f'{replace.size:>14}',\n",
    "        f'{100*np.sum(data.set_nan)/sub.shape[0]:>6.2f}',\n",
    "        f'{np.sum(data.was_nan):>13}',\n",
    "        f'{sub.shape[0]-replace.size:>13}'\n",
    "    )\n",
    "\n",
    "    return data, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.DataFrame({\n",
    "    \"all_data\":pd.concat([train, test])[training_logs].isna().sum(axis=0)/pd.concat([train, test])[training_logs].shape[0]*100,\n",
    "    \"train\":train_fe[feature_logs].isna().sum(axis=0)/train_fe.shape[0]*100,\n",
    "    \"test\":test_fe[feature_logs].isna().sum(axis=0)/test_fe.shape[0]*100\n",
    "})\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8dolH89oMWN"
   },
   "outputs": [],
   "source": [
    "imputation_train_dfs = dict()\n",
    "imputation_train_keys = ['DTC', 'DTS', 'RHOB']\n",
    "\n",
    "for key in imputation_train_keys:\n",
    "    imputation_train_dfs[key] = data_prep(train_fe.copy(), key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**because random is used to create the training gaps -> we might need to rethink this so the numbre of nan for training in each case stays relatively constant** Also, this approach troubles me a little because when logs are missing the missing sections in logs are usually associated with each other, *i.e.* missing dtse -> missing dte as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputaiton Models\n",
    "\n",
    "Various imputation models are tried:\n",
    "\n",
    " - LGBM with MICE (Random imputation order).\n",
    " - LGBM with MICE (Ascending number of missing imputation order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8ldhfJZsQ5B"
   },
   "outputs": [],
   "source": [
    "# training models\n",
    "\n",
    "imputation_args = dict(\n",
    "    # Random Order MICE - LGBM\n",
    "    lgbasc = dict(\n",
    "        training_set = feature_logs,\n",
    "        estimator = LGBMRegressor(random_state=456, n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=10, tol=0.01, imputation_order='ascending',)\n",
    "    ),\n",
    "    # Ascending Order MICE - LGBM\n",
    "    lgbasc = dict(\n",
    "        training_set = feature_logs,\n",
    "        estimator = LGBMRegressor(random_state=456, n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=10, tol=0.01, imputation_order='ascending',)\n",
    "    ),\n",
    "    # Bayesian Ridge Regression\n",
    "    brr = dict(\n",
    "        estimator = BayesianRidge(),\n",
    "        training_set = feature_logs,\n",
    "        kwargs=dict(random_state=456), \n",
    "    ),\n",
    "    brr1 = dict(\n",
    "        estimator = BayesianRidge(),\n",
    "        training_set = feature_logs,\n",
    "        kwargs=dict(random_state=456, max_iter=1, imputation_order=\"ascending\"), \n",
    "    ),\n",
    "# knn had to be disabled, I don't think the scikit learn one scales well to large data sets.\n",
    "#     knn = dict(\n",
    "#         training_set = feature_logs,\n",
    "#         estimator = KNeighborsRegressor(n_jobs=4),\n",
    "#         kwargs = dict(random_state=456, max_iter=10, tol=0.01)\n",
    "#     ),\n",
    "#     knn1 = dict(\n",
    "#         training_set = feature_logs,\n",
    "#         estimator = KNeighborsRegressor(n_jobs=4),\n",
    "#         kwargs = dict(random_state=456, max_iter=1, imputation_order=\"ascending\", tol=0.01)\n",
    "#     ),\n",
    ")\n",
    "\n",
    "def train_models(data, training_set=None, estimator=None, **kwargs):\n",
    "    \"\"\"Train a model using a MICE iterative imputer.\n",
    "    \"\"\"\n",
    "    # print(kwargs)\n",
    "    mice = IterativeImputer(estimator, **kwargs['kwargs'])\n",
    "    mice.fit(data[training_set])\n",
    "    data.loc[:, training_set] = mice.transform(data[training_set])\n",
    "    return data, mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8ldhfJZsQ5B"
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "imputed = dict()\n",
    "for imp_mod, args in imputation_args.items():\n",
    "# for imp_mod, args in [(key, imputation_args[key]) for key in [\"brr1\", \"knn1\"]]:\n",
    "    for key, val in imputation_train_dfs.items():\n",
    "        start = time.time()\n",
    "        imputed[imp_mod+'_'+key], models[imp_mod+'_'+key] = train_models(val[0].copy(), **args)\n",
    "        print(f\"{imp_mod} for {key} training complted in {time.time()-start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HN7zrTZ4c8r4"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate(data1, data2, j):\n",
    "    \"\"\"Evaluate the models against the NANed data from the training set.\n",
    "    \"\"\"\n",
    "    mask = data1.set_nan.values\n",
    "    truth = data2.loc[mask, j].values\n",
    "    test = data1.loc[mask, j].values\n",
    "    se = np.power((truth - test)/truth, 2)\n",
    "    score = np.nanmean(np.power(se, 0.5))*100.0\n",
    "    er = dict(\n",
    "        perc_error=score,\n",
    "        explained_var=metrics.explained_variance_score(truth, test),\n",
    "        max_error=metrics.max_error(truth, test),\n",
    "        mae=metrics.mean_absolute_error(truth, test),\n",
    "        mse=metrics.mean_squared_error(truth, test),\n",
    "        r2=metrics.r2_score(truth, test),\n",
    "    )\n",
    "    return er\n",
    "\n",
    "scores = defaultdict(dict)\n",
    "for key, d in imputed.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores[f'{key}_{mod}'] = evaluate(d, train_fe, key)\n",
    "\n",
    "mices_score = pd.DataFrame(scores)\n",
    "mices_score.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, figsize=(20, 10), sharex=True)\n",
    "mod = \"brr\"\n",
    "\n",
    "axs[0].plot(train_fe.DTC[imputed[f'{mod}_DTC'].set_nan].values)\n",
    "axs[0].plot(imputed[f\"{mod}_DTC\"].DTC[imputed[f'{mod}_DTC'].set_nan].values)\n",
    "axs[1].plot(train_fe.DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "axs[1].plot(imputed[f\"{mod}_DTS\"].DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "axs[2].plot(train_fe.RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "axs[2].plot(imputed[f\"{mod}_RHOB\"].RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "axs[3].plot(train_fe.isna().sum(axis=1)[imputed[f'{mod}_DTC'].set_nan].values)\n",
    "axs[3].plot(train_fe.isna().sum(axis=1)[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "axs[3].plot(train_fe.isna().sum(axis=1)[imputed[f'{mod}_RHOB'].set_nan].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=9, figsize=(15, 20), sharey=\"row\", sharex=\"col\")\n",
    "mod='lgbasc'\n",
    "logs = [ 'ZONE', 'GR', 'NPHI', 'PEF', 'RM10', 'RD10', 'DT', 'DTS', \"RHOB\"]\n",
    "for row, (well, val) in zip(range(4), train.groupby(\"WELL\")):\n",
    "    axs[row, -3].plot(imputed[f\"{mod}_DT\"].loc[val.index, \"DT\"], -val.TVDSS, 'g')\n",
    "    axs[row, -2].plot(imputed[f\"{mod}_DTS\"].loc[val.index, \"DTS\"], -val.TVDSS, 'g')\n",
    "    axs[row, -1].plot(imputed[f\"{mod}_RHOB\"].loc[val.index, \"RHOB\"], -val.TVDSS, 'g')\n",
    "    for col, log in enumerate(logs):\n",
    "        axs[row, col].plot(val[log], -val.TVDSS, 'k', alpha=0.5)\n",
    "for col, log in enumerate(logs):\n",
    "    axs[0, col].set_title(log)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Step LGBM\n",
    "\n",
    "Here we test the MICE imputation against a single imputation pass using LGBM. The inputs and the predictors are the same but multiple imputation chaining is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)\n",
    "dts_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)\n",
    "rho_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_col = feature_logs.copy()\n",
    "dt_col.remove(\"DTC\")\n",
    "dts_col = feature_logs.copy()\n",
    "dts_col.remove(\"DTS\")\n",
    "rhob_col = feature_logs.copy()\n",
    "rhob_col.remove(\"RHOB\")\n",
    "\n",
    "dt_lgbm.fit(imputation_train_dfs['DTC'][0].dropna(subset=['DTC']).loc[:, dt_col], imputation_train_dfs['DTC'][0].dropna(subset=['DTC'])['DTC'])\n",
    "dts_lgbm.fit(imputation_train_dfs['DTS'][0].dropna(subset=['DTS']).loc[:, dts_col], imputation_train_dfs['DTS'][0].dropna(subset=['DTS'])['DTS'])\n",
    "rho_lgbm.fit(imputation_train_dfs['RHOB'][0].dropna(subset=['RHOB']).loc[:, rhob_col], imputation_train_dfs['RHOB'][0].dropna(subset=['RHOB'])['RHOB'])\n",
    "\n",
    "dt_one_imputed = imputation_train_dfs['DTC'][0].copy()\n",
    "dts_one_imputed = imputation_train_dfs['DTS'][0].copy()\n",
    "rhob_one_imputed = imputation_train_dfs['RHOB'][0].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_one_imputed.loc[dt_one_imputed[\"DTC\"].isna(), \"DTC\"] = dt_lgbm.predict(imputation_train_dfs['DTC'][0][dt_col])[dt_one_imputed[\"DTC\"].isna().values]\n",
    "dts_one_imputed.loc[dts_one_imputed[\"DTS\"].isna(), \"DTS\"] = dts_lgbm.predict(imputation_train_dfs['DTS'][0][dts_col])[dts_one_imputed[\"DTS\"].isna().values]\n",
    "rhob_one_imputed.loc[rhob_one_imputed[\"RHOB\"].isna(), \"RHOB\"] = rho_lgbm.predict(imputation_train_dfs['RHOB'][0][rhob_col])[rhob_one_imputed[\"RHOB\"].isna().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mices_score[\"DTC_onepass\"] = evaluate(dt_one_imputed, train_fe, \"DTC\").values()\n",
    "mices_score[\"DTS_onepass\"] = evaluate(dts_one_imputed, train_fe, \"DTS\").values()\n",
    "mices_score[\"RHOB_onepass\"] = evaluate(rhob_one_imputed, train_fe, \"RHOB\").values()\n",
    "\n",
    "mices_score.sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the training results:\n",
    "\n",
    "  - KNN could not be run due to size of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed[\"lgbonce_DTC\"] = dt_one_imputed\n",
    "imputed[\"lgbonce_DTS\"] = dts_one_imputed\n",
    "imputed[\"lgbonce_RHOB\"] = rhob_one_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find metrics for predicting Y when X is missing\n",
    "missing_logs = ['DTC', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10']\n",
    "\n",
    "missing_scores = []\n",
    "for missing_log in missing_logs:\n",
    "    scores_when_missing = defaultdict(dict)\n",
    "    for key, d in imputed.items():\n",
    "        mod, key = key.split('_')\n",
    "        if missing_log == key:\n",
    "            scores_when_missing[f'{key}_{mod}'] = evaluate(d, train_fe, key)\n",
    "            scores_when_missing[f'{key}_{mod}']\n",
    "        else:\n",
    "\n",
    "            mask = train[missing_log].isna()\n",
    "            d = d.loc[~mask, :]\n",
    "            scores_when_missing[f'{key}_{mod}'] = evaluate(d, train_fe.loc[~mask, :], key)\n",
    "            scores_when_missing[f'{key}_{mod}']\n",
    "    temp_df = pd.DataFrame(scores_when_missing)\n",
    "    temp_df[\"always_present_log\"] = missing_log\n",
    "    missing_scores.append(temp_df)\n",
    "missing_scores = pd.concat(missing_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "melted = missing_scores.reset_index().melt(id_vars=[\"index\", \"always_present_log\"], var_name=\"model\").sort_values(\"model\")\n",
    "isnonemodel = melted.model.apply(lambda x: x.split(\"_\")[0]) == melted.always_present_log\n",
    "melted.loc[isnonemodel, \"always_present_log\"] = \"none\"\n",
    "g = sns.FacetGrid(melted, height=5, aspect=2, col=\"index\", col_wrap=2, sharey=False)\n",
    "g.map_dataframe(sns.barplot, x=\"model\", y=\"value\", hue=\"always_present_log\", palette=\"pastel\",  hue_order=[\"none\", 'DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'])\n",
    "g.add_legend(title=\"Log has no\\nNan Values\")\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation\n",
    "\n",
    "Here we use the trained models on the blind well test sets to see if the predictive accuracy is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_test_dfs = dict()\n",
    "imputation_test_keys = ['DTC', 'DTS', 'RHOB']\n",
    "\n",
    "for key in imputation_test_keys:\n",
    "    imputation_test_dfs[key] = data_prep(test_fe.copy(), key, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute test sets\n",
    "imputed_test = dict()\n",
    "for imp_mod, args in imputation_args.items():\n",
    "    for key, val in imputation_test_dfs.items():\n",
    "        temp_df = val[0].copy()\n",
    "        temp_df.loc[:, args[\"training_set\"]] = models[imp_mod+'_'+key].transform(val[0][args[\"training_set\"]])\n",
    "        imputed_test[imp_mod+'_'+key] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_lgbm.predict(imputation_test_dfs['DTC'][0][dt_col]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_one_imputed_test[\"DTC\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute tests sets using lgbonce models\n",
    "dt_one_imputed_test = imputation_test_dfs['DTC'][0].copy()\n",
    "dts_one_imputed_test = imputation_test_dfs['DTS'][0].copy()\n",
    "rhob_one_imputed_test = imputation_test_dfs['RHOB'][0].copy()\n",
    "# dt_one_imputed_test.loc[dt_one_imputed_test[\"DTC\"].isna(), \"DTC\"] = dt_lgbm.predict(imputation_test_dfs['DTC'][0][dt_col])[dt_one_imputed_test[\"DTC\"].isna().values]\n",
    "# dts_one_imputed_test.loc[dts_one_imputed_test[\"DTS\"].isna(), \"DTS\"] = dts_lgbm.predict(imputation_test_dfs['DTS'][0][dts_col])[dts_one_imputed_test[\"DTS\"].isna().values]\n",
    "# rhob_one_imputed_test.loc[rhob_one_imputed_test[\"RHOB\"].isna(), \"RHOB\"] = rho_lgbm.predict(imputation_test_dfs['RHOB'][0][rhob_col])[rhob_one_imputed_test[\"RHOB\"].isna().values]\n",
    "dt_one_imputed_test[\"DTC\"] = dt_lgbm.predict(imputation_test_dfs['DTC'][0][dt_col])\n",
    "dts_one_imputed_test[\"DTS\"] = dts_lgbm.predict(imputation_test_dfs['DTS'][0][dts_col])\n",
    "rhob_one_imputed_test[\"RHOB\"] = rho_lgbm.predict(imputation_test_dfs['RHOB'][0][rhob_col])\n",
    "\n",
    "imputed_test[\"lgbonce_DTC\"] = dt_one_imputed_test\n",
    "imputed_test[\"lgbonce_DTS\"] = dts_one_imputed_test\n",
    "imputed_test[\"lgbonce_RHOB\"] = rhob_one_imputed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = dict()\n",
    "for key, d in imputed_test.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores_test[f'{key}_{mod}'] = evaluate(d, test_fe, key)\n",
    "pd.DataFrame(scores_test).sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "for mod, name in zip(['lgbrand', 'brr', 'brr1', 'lgbonce'], ['GBT Random', 'Bayesian Ridge Regression', 'Bayesian Ridge Regression Once', \"GBT Direct\"]):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, gridspec_kw={\"width_ratios\":[9, 1]}, figsize=(15, 7), sharex=\"col\")\n",
    "    \n",
    "    gs = axs[2, 1].get_gridspec()\n",
    "    # remove the underlying axes\n",
    "    for ax in axs[:, 1]:\n",
    "        ax.remove()\n",
    "    cbar_ax = fig.add_subplot(gs[1:, -1])\n",
    "    \n",
    "    \n",
    "#     cb = axs[:, 1]\n",
    "    axs = axs[:, 0]\n",
    "    axs[0].plot(imputed_test[f\"{mod}_DTC\"].DTC.values)\n",
    "    axs[0].plot(test_fe.DTC.values)\n",
    "    axs0_2 = axs[0].twinx()\n",
    "    axs0_2.grid(False)\n",
    "    axs0_2.plot(test_fe.Grp_TSS_DTC.values, 'k')\n",
    "    ylabels = ['{:,.0f}'.format(x) + 'K' for x in axs0_2.get_yticks()/1000]\n",
    "    axs0_2.set_yticklabels(ylabels)\n",
    "    axs[0].set_ylabel(\"DTC\")\n",
    "    axs[1].plot(imputed_test[f\"{mod}_DTS\"].DTS.values)\n",
    "    axs[1].plot(test_fe.DTS.values)\n",
    "    axs[1].set_ylabel(\"DTS\")\n",
    "    axs1_2 = axs[1].twinx()\n",
    "    axs1_2.grid(False)\n",
    "    axs1_2.plot(test_fe.Grp_TSS_DTS.values, 'k')\n",
    "    ylabels = ['{:,.0f}'.format(x) + 'K' for x in axs1_2.get_yticks()/1000]\n",
    "    axs1_2.set_yticklabels(ylabels)\n",
    "    axs[2].plot(imputed_test[f\"{mod}_RHOB\"].RHOB.values)\n",
    "    axs[2].plot(test_fe.RHOB.values)\n",
    "    axs[2].set_ylabel(\"RHOB\")\n",
    "    axs2_2 = axs[2].twinx()\n",
    "    axs2_2.grid(False)\n",
    "    axs2_2.plot(test_fe.Grp_TSS_RHOB.values, 'k')\n",
    "    ylabels = ['{:,.0f}'.format(x) + 'K' for x in axs2_2.get_yticks()/1000]\n",
    "    axs2_2.set_yticklabels(ylabels)\n",
    "\n",
    "    test_n = test_fe/test_fe\n",
    "    test_n = test_n[feature_logs]\n",
    "    test_n = test_n[sorted(test_n.columns)]\n",
    "    for i, col in enumerate(test_n.columns):\n",
    "        test_n[col] = (i)\n",
    "        test_n.loc[~test_fe[col].isna().values, col] = np.nan\n",
    "        \n",
    "    missing = axs[3].imshow(test_n.T.values, aspect='auto', cmap=cm.get_cmap('tab20', test_n.shape[1]), interpolation='none', origin='lower', vmin=0, vmax=test_n.shape[1])\n",
    "    axs[3].set_ylabel(\"Missing Features\")\n",
    "    axs[3].set_xlabel(\"Sample\")\n",
    "    axs[0].set_title(f\"Imputation Validation Samples for {name}\", fontsize=12)\n",
    "    axs[3].set_yticklabels([])\n",
    "    axs[3].grid(False)\n",
    "    plt.colorbar(missing, cax=cbar_ax,)\n",
    "    cbar_ax.set_yticks([], minor=False, major=False)\n",
    "    cbar_ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    for lab, v in zip(sorted(test_n.columns.to_list()), np.linspace(0.2, 18.2, test_n.shape[1])):\n",
    "        cbar_ax.text(-0.5, v, lab, fontsize=9, ha='right')\n",
    "    cbar_ax.set_title(\"Missing Features\")\n",
    "    fig.savefig(f'figures/qual_test_f20_{mod}.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zoom = slice(29530,31000)\n",
    "\n",
    "for mod, name in zip(['lgbrand', 'brr', 'brr1', 'lgbonce'], ['GBT Random', 'Bayesian Ridge Regression', 'Bayesian Ridge Regression Once', \"GBT Direct\"]):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, gridspec_kw={\"width_ratios\":[9, 1]}, figsize=(15, 7), sharex=\"col\")\n",
    "    \n",
    "    gs = axs[2, 1].get_gridspec()\n",
    "    # remove the underlying axes\n",
    "    for ax in axs[:, 1]:\n",
    "        ax.remove()\n",
    "    cbar_ax = fig.add_subplot(gs[1:, -1])\n",
    "    \n",
    "    \n",
    "#     cb = axs[:, 1]\n",
    "    axs = axs[:, 0]\n",
    "    axs[0].plot(imputed_test[f\"{mod}_DTC\"].DTC.values[zoom])\n",
    "    axs[0].plot(test_fe.DTC.values[zoom])\n",
    "    axs[0].set_ylabel(\"DTC\")\n",
    "    axs[1].plot(imputed_test[f\"{mod}_DTS\"].DTS.values[zoom])\n",
    "    axs[1].plot(test_fe.DTS.values[zoom])\n",
    "    axs[1].set_ylabel(\"DTS\")\n",
    "    axs[2].plot(imputed_test[f\"{mod}_RHOB\"].RHOB.values[zoom])\n",
    "    axs[2].plot(test_fe.RHOB.values[zoom])\n",
    "    axs[2].set_ylabel(\"RHOB\")\n",
    "\n",
    "    test_n = test_fe/test_fe\n",
    "    test_n = test_n[feature_logs]\n",
    "    test_n = test_n[sorted(test_n.columns)]\n",
    "    for i, col in enumerate(test_n.columns):\n",
    "        test_n[col] = (i)\n",
    "        test_n.loc[~test_fe[col].isna().values, col] = np.nan\n",
    "        \n",
    "    missing = axs[3].imshow(test_n.T.values[:, zoom], aspect='auto', cmap=cm.get_cmap('tab20', test_n.shape[1]), interpolation='none', origin='lower', vmin=0, vmax=test_n.shape[1])\n",
    "    axs[3].set_ylabel(\"Missing Features\")\n",
    "    axs[3].set_xlabel(\"Sample\")\n",
    "    axs[0].set_title(f\"Imputation Validation Samples for {name}\", fontsize=12)\n",
    "    axs[3].set_yticklabels([])\n",
    "    axs[3].grid(False)\n",
    "    plt.colorbar(missing, cax=cbar_ax,)\n",
    "    cbar_ax.set_yticks([], minor=False, major=False)\n",
    "    cbar_ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    for lab, v in zip(sorted(test_n.columns.to_list()), np.linspace(0.2, 18.2, test_n.shape[1])):\n",
    "        cbar_ax.text(-0.5, v, lab, fontsize=9, ha='right')\n",
    "    cbar_ax.set_title(\"Missing Features\")\n",
    "    break\n",
    "    axs[2].set_xlim(20000, 30000)\n",
    "    #     fig.savefig(f'figures/qual_test_f20_{mod}.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Othere Error tests on per zone basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_er_dfs = []\n",
    "for fm, val in test_fe.groupby('FORMATIONe'):\n",
    "    scores_test_fm = dict()\n",
    "    for key, d in imputed_test.items():\n",
    "        mod, key = key.split('_')\n",
    "        try:\n",
    "            ev = evaluate(d.loc[val.index, :], val, key)\n",
    "            ev[\"zone\"] = fm\n",
    "            ev[\"Training Samples\"] = val[f\"Fm_TSS_{key}\"].mean()\n",
    "            ev[\"Imputed\"] = key\n",
    "            scores_test_fm[f'{key}_{mod}'] = ev\n",
    "            zone_er_dfs.append(\n",
    "                pd.DataFrame(scores_test_fm).sort_index(axis=1).T.round(2)\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "er_loc = pd.concat(zone_er_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=er_loc, x='zone', y='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, sharex=True, figsize=(10, 8))\n",
    "\n",
    "sns.scatterplot(data=er_loc.loc[[x for x in er_loc.index.unique() if 'lgbrand' in x], :], x='Training Samples', y='mse', hue='Imputed', ax=axs[0, 0])\n",
    "axs[0, 0].set_ylim(0, 1.1)\n",
    "axs[0, 0].set_ylabel(\"MSE\")\n",
    "\n",
    "sns.scatterplot(data=er_loc.loc[[x for x in er_loc.index.unique() if 'lgbrand' in x], :], x='Training Samples', y='mae', hue='Imputed', ax=axs[0, 1])\n",
    "axs[0, 1].set_ylim(0, 1.1)\n",
    "axs[0, 1].set_ylabel(\"MAE\")\n",
    "\n",
    "sns.scatterplot(data=er_loc.loc[[x for x in er_loc.index.unique() if 'lgbrand' in x], :], x='Training Samples', y='r2', hue='Imputed', ax=axs[1, 0])\n",
    "axs[1, 0].set_ylim(0, 1.1)\n",
    "axs[1, 0].set_ylabel(r\"$R^2$\")\n",
    "xlabels = ['{:,.0f}'.format(x) + 'K' for x in axs[1, 0].get_xticks()/1000]\n",
    "axs[1, 0].set_xticklabels(xlabels)\n",
    "\n",
    "sns.scatterplot(data=er_loc.loc[[x for x in er_loc.index.unique() if 'lgbrand' in x], :], x='Training Samples', y='explained_var', hue='Imputed', ax=axs[1, 1])\n",
    "axs[1, 1].set_ylim(0, 1.1)\n",
    "axs[1, 1].set_ylabel(\"Explained Variance\")\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"figures/f20_er_by_formation_lgbrand.png\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in er_loc.index.unique() if 'lgbrand' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "missing value prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-logs",
   "language": "python",
   "name": "ml-logs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
