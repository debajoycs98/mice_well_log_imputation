{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volve Well Log Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuXerCODbXmG"
   },
   "source": [
    "Firstly we try preparing the data and importing the liabraries important for the problem statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADZhgpaOblJ1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project has been sourced from the Volve Field opendataset provided by Equinor in 2018. There are 19 wells with 187,396 samples below the Hordaland Group Formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The zone map can convert the ZONE log of ints to named Zones.\n",
    "zone_map = {\n",
    "0: 'Seabed',\n",
    "1: 'NORDLAND',\n",
    "2: 'Utsira',\n",
    "3: 'HORDALAND',\n",
    "4: 'Ty',\n",
    "5: 'SHETLAND',\n",
    "6: 'Ekofisk',\n",
    "7: 'Hod',\n",
    "8: 'Draupne',\n",
    "9: 'Heather Shale',\n",
    "10: 'Heather Sand',\n",
    "11: 'Hugin C',\n",
    "12: 'Hugin B3',\n",
    "13: 'Hugin B2',\n",
    "14: 'Hugin B1',\n",
    "15: 'Hugin A',\n",
    "16: 'Sleipner',\n",
    "17: 'Skagerrak',\n",
    "18: 'Smith Bank'\n",
    "}\n",
    "\n",
    "# Lets rename logs in the HDF5 input to more common terms.\n",
    "col_rename_map = {\n",
    "    'ZONE_NO':'ZONE',\n",
    "    'DTE':'DT',\n",
    "    'DTSE':'DTS',\n",
    "    'DRHOE':'DRHO',\n",
    "    'GRE':'GR',\n",
    "    'NPHIE':'NPHI',\n",
    "    'PEFE':'PEF',\n",
    "    'RHOBE':'RHOB',\n",
    "    'RME':'RM',\n",
    "    'RSE':'RS',\n",
    "    'RDE':'RD',\n",
    "    \"WELL\": \"WELL_ID\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "id": "y5a-x-zKf9XS",
    "outputId": "0e7f0a1f-d7e5-4982-dd45-469bf165cbc5"
   },
   "outputs": [],
   "source": [
    "# Load data and perform encoding of labels.\n",
    "\n",
    "data = pd.read_hdf('data/volve_ml_logs.hdf5').rename(col_rename_map, axis=1)\n",
    "# look at deeper zones -> shallow zones poorly sampled/not of interest\n",
    "data = data.query(\"ZONE>=4\")\n",
    "data['ZONE'] = data[\"ZONE\"].astype(int)\n",
    "data[\"ZONE_NAME\"] = data[\"ZONE\"].map(zone_map)\n",
    "\n",
    "\n",
    "well_name_encoder=LabelEncoder()\n",
    "data['WELL']=well_name_encoder.fit_transform(data['WELL_ID'])\n",
    "display(data.head(5))\n",
    "display(\"Samples, Features\", data.shape)\n",
    "# How many wells are there?\n",
    "display(\"Wells N:\", data.WELL.max() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Statistics\n",
    "The number of missing values and missing values per well and per zone.\n",
    "\n",
    "Wells where key logs DTE, DTSE and RHOBE are missing? Are there any trends here, how might the distribution of missing data affect our imputation performance? **In your accuracy score Debajoy it would be useful to do some deeper analysis to see if any zones perform better than others, an important part of this process is to understand the limitations of the algorithm you are using and to know when and where it will and won't work -> and for a paper, most importantly, why.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pc = pd.DataFrame({'ALL':data.count()/data.shape[0]})\n",
    "for well, sub in data.groupby('WELL_ID'):\n",
    "    missing_pc[well] = 1- sub.count()/sub.shape[0]\n",
    "for zn, sub in data.groupby('ZONE'):\n",
    "    missing_pc[zone_map[zn]] = 1- sub.count()/sub.shape[0]\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "plasma_r5 = mpl.cm.get_cmap('plasma_r', 11)\n",
    "plasma_r5.colors[0] = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(missing_pc.T.iloc[:20, :10], cbar_kws={'label':'Fraction Missing'}, cmap=plasma_r5, vmin=-0.1, vmax=1)\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/well_fraction_missing.png', dpi=150)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(missing_pc.T.iloc[20:, :10], cbar_kws={'label':'Fraction Missing'}, cmap=plasma_r5, vmin=-0.1, vmax=1)\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/zone_fraction_missing.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgU_iNyDp7HP"
   },
   "source": [
    "## Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "DnG_YSgWm-Mz",
    "outputId": "41216a60-c26b-4061-91ff-9f179c91a32d"
   },
   "outputs": [],
   "source": [
    "# dropping rows that are all NA (i.e. 10 of the logs are all nan)\n",
    "data_fe = data.dropna(thresh=10).copy()\n",
    "print(data_fe.shape)\n",
    "\n",
    "data_fe['RD10'] = np.log10(data_fe['RD'])\n",
    "data_fe['RM10'] = np.log10(data_fe['RM'])\n",
    "\n",
    "# remove label columns\n",
    "drop = [\"WELL_ID\", \"ZONE_NAME\"]\n",
    "data_fe_labels = data_fe[drop].copy()\n",
    "data_fe = data_fe.drop(drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind Wells Train Test Split\n",
    "\n",
    "Here we remove wells F-4, F-12 and F-1 from the training dataset.\n",
    "\n",
    "Try to select wells that preserve the split of missing values across the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wells = well_name_encoder.transform([\"F-4\", \"F-12\", \"F-1\", \"F-15D\"])\n",
    "train = data_fe[~data_fe.WELL.isin(test_wells)].copy()\n",
    "test = data_fe[data_fe.WELL.isin(test_wells)].copy()\n",
    "test_noedits = test.copy()\n",
    "# introduce some missing elastic values to test\n",
    "test[\"RHOB\"].iloc[25000:31000] = np.nan\n",
    "test[\"DT\"].iloc[25000:31000] = np.nan\n",
    "test[\"DTS\"].iloc[25000:31000] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "print(train.size, test.size, test.shape[0]/train.shape[0])\n",
    "print(pd.DataFrame({\"train\": 1 - train.count()/train.shape[0], \"test\": 1 - test.count()/test.shape[0]}).T)\n",
    "\n",
    "# calculate a scalar on train and apply to both\n",
    "sscaler = StandardScaler()\n",
    "sscaler.fit(data_fe)\n",
    "\n",
    "train.loc[:, :] = sscaler.transform(train)\n",
    "test.loc[:, :] = sscaler.transform(test)\n",
    "test_noedits.loc[:, :] = sscaler.transform(test_noedits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = msno.matrix(data.iloc[:, :10], figsize=(15, 7))\n",
    "plt.savefig('figures/volve_missingno.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = msno.matrix(train.iloc[:, :10], figsize=(15, 7))\n",
    "plt.savefig('figures/volve_missingno_train.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = msno.matrix(test.iloc[:, :10], figsize=(15, 7))\n",
    "plt.savefig('figures/volve_missingno_Test.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data, j, set_to_nan=0.3):\n",
    "    \"\"\"This method sets set_to_nan fraction of the values to nan so we can measure the model accuracy.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    sub = data.dropna(subset=[j])\n",
    "    rand_set_mask = np.random.random(len(sub)) < set_to_nan\n",
    "    replace = sub.index[rand_set_mask]\n",
    "    data.loc[replace, j] = np.nan\n",
    "    data['set_nan'] = False\n",
    "    data.loc[replace, 'set_nan'] = True\n",
    "    data['was_nan'] = data[j].isna()\n",
    "    print('Col, InputSize, Number of Nan, % NaN, Original Nan', 'Training Size')\n",
    "    print(\n",
    "        f'{j:>3}',\n",
    "        f'{data.shape[0]:>10}',\n",
    "        f'{replace.size:>14}',\n",
    "        f'{100*np.sum(data.set_nan)/sub.shape[0]:>6.2f}',\n",
    "        f'{np.sum(data.was_nan):>13}',\n",
    "        f'{sub.shape[0]-replace.size:>13}'\n",
    "    )\n",
    "\n",
    "    return data, replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.DataFrame({\n",
    "    \"all_data\":data_fe.isna().sum(axis=0)/data_fe.shape[0]*100,\n",
    "    \"train\":train.isna().sum(axis=0)/data_fe.shape[0]*100,\n",
    "    \"test\":test.isna().sum(axis=0)/data_fe.shape[0]*100\n",
    "})\n",
    "missing.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8dolH89oMWN"
   },
   "outputs": [],
   "source": [
    "imputation_train_dfs = dict()\n",
    "imputation_train_keys = ['DT', 'DTS', 'RHOB']\n",
    "\n",
    "for key in imputation_train_keys:\n",
    "    imputation_train_dfs[key] = data_prep(train.copy(), key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**because random is used to create the training gaps -> we might need to rethink this so the numbre of nan for training in each case stays relatively constant** Also, this approach troubles me a little because when logs are missing the missing sections in logs are usually associated with each other, *i.e.* missing dtse -> missing dte as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputaiton Models\n",
    "\n",
    "Various imputation models are tried:\n",
    "\n",
    " - LGBM with MICE (Random imputation order).\n",
    " - LGBM with MICE (Ascending number of missing imputation order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8ldhfJZsQ5B"
   },
   "outputs": [],
   "source": [
    "# training models\n",
    "\n",
    "imputation_args = dict(\n",
    "    # Random Order MICE - LGBM\n",
    "    lgbrand = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10', 'ZONE'],\n",
    "        estimator = LGBMRegressor(n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=20, tol=0.01, imputation_order='random',)\n",
    "    ),\n",
    "    # Ascending Order MICE - LGBM\n",
    "    lgbasc = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10', 'ZONE'],\n",
    "        estimator = LGBMRegressor(random_state=456, n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=20, tol=0.01, imputation_order='ascending',)\n",
    "    ),\n",
    "    # Bayesian Ridge Regression\n",
    "    brr = dict(\n",
    "        estimator = BayesianRidge(),\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'],\n",
    "        kwargs=dict(random_state=456), \n",
    "    ),\n",
    "    brr1 = dict(\n",
    "        estimator = BayesianRidge(),\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'],\n",
    "        kwargs=dict(random_state=456, max_iter=1, imputation_order=\"ascending\"), \n",
    "    ),\n",
    "    knn = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'],\n",
    "        estimator = KNeighborsRegressor(n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=20, tol=0.01)\n",
    "    ),\n",
    "    knn1 = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'],\n",
    "        estimator = KNeighborsRegressor(n_jobs=4),\n",
    "        kwargs = dict(random_state=456, max_iter=1, imputation_order=\"ascending\", tol=0.01)\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "imputation_args2 = dict(\n",
    "    # Random Order MICE - LGBM\n",
    "    lgbrand = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10', 'ZONE'],\n",
    "        estimator = LGBMRegressor(n_jobs=4, num_leaves=100, max_depth=5),\n",
    "        kwargs = dict(random_state=456, max_iter=20, tol=0.01, imputation_order='random')\n",
    "    ),\n",
    "    # Ascending Order MICE - LGBM\n",
    "    lgbasc = dict(\n",
    "        training_set = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10', 'ZONE'],\n",
    "        estimator = LGBMRegressor(random_state=456, n_jobs=4, num_leaves=100, max_depth=5),\n",
    "        kwargs = dict(random_state=456, max_iter=20, tol=0.01, imputation_order='ascending',)\n",
    "    ),\n",
    ")\n",
    "\n",
    "def train_models(data, training_set=None, estimator=None, **kwargs):\n",
    "    \"\"\"Train a model using a MICE iterative imputer.\n",
    "    \"\"\"\n",
    "    # print(kwargs)\n",
    "    mice = IterativeImputer(estimator, **kwargs['kwargs'])\n",
    "    mice.fit(data[training_set])\n",
    "    data.loc[:, training_set] = mice.transform(data[training_set])\n",
    "    return data, mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8ldhfJZsQ5B"
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "imputed = dict()\n",
    "for imp_mod, args in imputation_args.items():\n",
    "# for imp_mod, args in [(key, imputation_args[key]) for key in [\"brr1\", \"knn1\"]]:\n",
    "\n",
    "    for key, val in imputation_train_dfs.items():\n",
    "        imputed[imp_mod+'_'+key], models[imp_mod+'_'+key] = train_models(val[0].copy(), **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add here some models where we have broader trees, this seemed to work better with direct imputers.\n",
    "models2 = dict()\n",
    "imputed2 = dict()\n",
    "for imp_mod, args in imputation_args2.items():\n",
    "    for key, val in imputation_train_dfs.items():\n",
    "        imputed2[imp_mod+'_'+key], models2[imp_mod+'_'+key] = train_models(val[0].copy(), **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HN7zrTZ4c8r4"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate(data1, data2, j):\n",
    "    \"\"\"Evaluate the models against the NANed data from the training set.\n",
    "    \"\"\"\n",
    "    mask = data1.set_nan.values\n",
    "    truth = data2.loc[mask, j].values\n",
    "    test = data1.loc[mask, j].values\n",
    "    se = np.power((truth - test)/truth, 2)\n",
    "    score = np.nanmean(np.power(se, 0.5))*100.0\n",
    "    er = dict(\n",
    "        perc_error=score,\n",
    "        explained_var=metrics.explained_variance_score(truth, test),\n",
    "        max_error=metrics.max_error(truth, test),\n",
    "        mae=metrics.mean_absolute_error(truth, test),\n",
    "        mse=metrics.mean_squared_error(truth, test),\n",
    "        r2=metrics.r2_score(truth, test),\n",
    "    )\n",
    "    return er\n",
    "\n",
    "scores = defaultdict(dict)\n",
    "for key, d in imputed.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores[f'{key}_{mod}'] = evaluate(d, train, key)\n",
    "\n",
    "mices_score = pd.DataFrame(scores)\n",
    "mices_score.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = defaultdict(dict)\n",
    "for key, d in imputed2.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores2[f'{key}_{mod}'] = evaluate(d, train, key)\n",
    "\n",
    "mices_score2 = pd.DataFrame(scores2)\n",
    "mices_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, figsize=(15, 7), sharex=True)\n",
    "mod = \"lgbasc\"\n",
    "\n",
    "axs[0].plot(train.DT[imputed[f'{mod}_DT'].set_nan].values)\n",
    "axs[0].plot(imputed[f\"{mod}_DT\"].DT[imputed[f'{mod}_DT'].set_nan].values)\n",
    "axs[0].set_ylabel(\"DT\")\n",
    "axs[1].plot(train.DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "axs[1].plot(imputed[f\"{mod}_DTS\"].DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "axs[1].set_ylabel(\"DTS\")\n",
    "axs[2].plot(train.RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "axs[2].plot(imputed[f\"{mod}_RHOB\"].RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "axs[2].set_ylabel(\"RHOB\")\n",
    "axs[3].plot(train.isna().sum(axis=1)[imputed[f'{mod}_DT'].set_nan].values, label=\"DT\")\n",
    "axs[3].plot(train.isna().sum(axis=1)[imputed[f'{mod}_DTS'].set_nan].values, label=\"DTS\")\n",
    "axs[3].plot(train.isna().sum(axis=1)[imputed[f'{mod}_RHOB'].set_nan].values, label=\"RHOB\")\n",
    "axs[3].legend()\n",
    "axs[3].set_ylabel(\"Missing Features\")\n",
    "axs[3].set_xlabel(\"Sample\")\n",
    "axs[0].set_title(\"Imputation Validation Samples\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=9, figsize=(15, 20), sharey=\"row\", sharex=\"col\")\n",
    "mod='lgbasc'\n",
    "logs = [ 'ZONE', 'GR', 'NPHI', 'PEF', 'RM10', 'RD10', 'DT', 'DTS', \"RHOB\"]\n",
    "for row, (well, val) in zip(range(4), train.groupby(\"WELL\")):\n",
    "    axs[row, -3].plot(imputed[f\"{mod}_DT\"].loc[val.index, \"DT\"], -val.TVDSS, 'g')\n",
    "    axs[row, -2].plot(imputed[f\"{mod}_DTS\"].loc[val.index, \"DTS\"], -val.TVDSS, 'g')\n",
    "    axs[row, -1].plot(imputed[f\"{mod}_RHOB\"].loc[val.index, \"RHOB\"], -val.TVDSS, 'g')\n",
    "    for col, log in enumerate(logs):\n",
    "        axs[row, col].plot(val[log], -val.TVDSS, 'k', alpha=0.5)\n",
    "for col, log in enumerate(logs):\n",
    "    axs[0, col].set_title(log)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Step LGBM\n",
    "\n",
    "Here we test the MICE imputation against a single imputation pass using LGBM. The inputs and the predictors are the same but multiple imputation chaining is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)\n",
    "dts_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)\n",
    "rho_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_col = ['DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "dts_col = ['DT', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "rhob_col = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RM10', 'ZONE',]\n",
    "\n",
    "dt_lgbm.fit(imputation_train_dfs['DT'][0].dropna(subset=['DT']).loc[:, dt_col], imputation_train_dfs['DT'][0].dropna(subset=['DT'])['DT'])\n",
    "dts_lgbm.fit(imputation_train_dfs['DTS'][0].dropna(subset=['DTS']).loc[:, dts_col], imputation_train_dfs['DTS'][0].dropna(subset=['DTS'])['DTS'])\n",
    "rho_lgbm.fit(imputation_train_dfs['RHOB'][0].dropna(subset=['RHOB']).loc[:, rhob_col], imputation_train_dfs['RHOB'][0].dropna(subset=['RHOB'])['RHOB'])\n",
    "\n",
    "dt_one_imputed = imputation_train_dfs['DT'][0].copy()\n",
    "dts_one_imputed = imputation_train_dfs['DTS'][0].copy()\n",
    "rhob_one_imputed = imputation_train_dfs['RHOB'][0].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_one_imputed.loc[dt_one_imputed[\"DT\"].isna(), \"DT\"] = dt_lgbm.predict(imputation_train_dfs['DT'][0][dt_col])[dt_one_imputed[\"DT\"].isna().values]\n",
    "dts_one_imputed.loc[dts_one_imputed[\"DTS\"].isna(), \"DTS\"] = dts_lgbm.predict(imputation_train_dfs['DTS'][0][dts_col])[dts_one_imputed[\"DTS\"].isna().values]\n",
    "rhob_one_imputed.loc[rhob_one_imputed[\"RHOB\"].isna(), \"RHOB\"] = rho_lgbm.predict(imputation_train_dfs['RHOB'][0][rhob_col])[rhob_one_imputed[\"RHOB\"].isna().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mices_score[\"DT_onepass\"] = evaluate(dt_one_imputed, train, \"DT\").values()\n",
    "mices_score[\"DTS_onepass\"] = evaluate(dts_one_imputed, train, \"DTS\").values()\n",
    "mices_score[\"RHOB_onepass\"] = evaluate(rhob_one_imputed, train, \"RHOB\").values()\n",
    "\n",
    "mices_score.sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the training results:\n",
    "\n",
    " - KNN and LGB tend to be pretty equivalent in terms of results.\n",
    " - The R2 fit for BRR in particular tends to be much lower than the other models.\n",
    " - One pass models are similar or better than MICE models suggesting in this case there is no benefit to running MICE style imputation.\n",
    " - All data types have R2 scores greater than 90% when using KNN or GB-Tree models. \n",
    " - Random or Ascending order had very little impact upon the prediction capabilities of GB-Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scores when certain logs are absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed[\"lgbonce_DT\"] = dt_one_imputed\n",
    "imputed[\"lgbonce_DTS\"] = dts_one_imputed\n",
    "imputed[\"lgbonce_RHOB\"] = rhob_one_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTS when DT and RHOB are absent.\n",
    "imputed[\"lgbonce_DTS\"][np.logical_and(imputed[\"lgbonce_DTS\"][\"DT\"].isna(), imputed[\"lgbonce_DTS\"][\"RHOB\"].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find metrics for predicting Y when X is missing\n",
    "missing_logs = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10']\n",
    "\n",
    "missing_scores = []\n",
    "for missing_log in missing_logs:\n",
    "    scores_when_missing = defaultdict(dict)\n",
    "    for key, d in imputed.items():\n",
    "        mod, key = key.split('_')\n",
    "        if missing_log == key:\n",
    "            scores_when_missing[f'{key}_{mod}'] = evaluate(d, train, key)\n",
    "            scores_when_missing[f'{key}_{mod}']\n",
    "        else:\n",
    "\n",
    "            mask = train[missing_log].isna()\n",
    "            d = d.loc[~mask, :]\n",
    "            scores_when_missing[f'{key}_{mod}'] = evaluate(d, train.loc[~mask, :], key)\n",
    "            scores_when_missing[f'{key}_{mod}']\n",
    "    temp_df = pd.DataFrame(scores_when_missing)\n",
    "    temp_df[\"always_present_log\"] = missing_log\n",
    "    missing_scores.append(temp_df)\n",
    "missing_scores = pd.concat(missing_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Plots for all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "melted = missing_scores.reset_index().melt(id_vars=[\"index\", \"always_present_log\"], var_name=\"model\").sort_values(\"model\")\n",
    "isnonemodel = melted.model.apply(lambda x: x.split(\"_\")[0]) == melted.always_present_log\n",
    "melted.loc[isnonemodel, \"always_present_log\"] = \"none\"\n",
    "g = sns.FacetGrid(melted, height=5, aspect=2, col=\"index\", col_wrap=2, sharey=False)\n",
    "g.map_dataframe(sns.barplot, x=\"model\", y=\"value\", hue=\"always_present_log\", palette=\"pastel\",  hue_order=[\"none\", 'DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RHOB', 'RM10', 'RD10'])\n",
    "g.add_legend(title=\"Log has no\\nNan Values\")\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from matplotlib import gridspec\n",
    "\n",
    "for mod, name in zip(['lgbrand', 'knn', 'knn1', 'brr', 'brr1', 'lgbonce'], ['GBT Random', 'KNN Regresssion', 'KNN Regresssion Once', 'Bayesian Ridge Regression', 'Bayesian Ridge Regression Once', \"GBT Direct\"]):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, gridspec_kw={\"width_ratios\":[9, 1]}, figsize=(15, 7), sharex=\"col\")\n",
    "    \n",
    "    gs = axs[2, 1].get_gridspec()\n",
    "    # remove the underlying axes\n",
    "    for ax in axs[:, 1]:\n",
    "        ax.remove()\n",
    "    cbar_ax = fig.add_subplot(gs[2:, -1])\n",
    "    \n",
    "    \n",
    "#     cb = axs[:, 1]\n",
    "    axs = axs[:, 0]\n",
    "    axs[0].plot(train.DT[imputed[f'{mod}_DT'].set_nan].values)\n",
    "    axs[0].plot(imputed[f\"{mod}_DT\"].DT[imputed[f'{mod}_DT'].set_nan].values)\n",
    "    axs[0].set_ylabel(\"DT\")\n",
    "    axs[1].plot(train.DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "    axs[1].plot(imputed[f\"{mod}_DTS\"].DTS[imputed[f'{mod}_DTS'].set_nan].values)\n",
    "    axs[1].set_ylabel(\"DTS\")\n",
    "    axs[2].plot(train.RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "    axs[2].plot(imputed[f\"{mod}_RHOB\"].RHOB[imputed[f'{mod}_RHOB'].set_nan].values)\n",
    "    axs[2].set_ylabel(\"RHOB\")\n",
    "\n",
    "    train_n = train/train\n",
    "    for i, col in enumerate(train_n.columns):\n",
    "        train_n[col] = (i + 1)\n",
    "        train_n.loc[~train[col].isna().values, col] = np.nan\n",
    "        \n",
    "    missing = axs[3].imshow(train_n.T.values, aspect='auto', cmap=cm.get_cmap('tab20', train_n.shape[1]), interpolation='none', origin='lower')\n",
    "    axs[3].set_ylabel(\"Missing Features\")\n",
    "    axs[3].set_xlabel(\"Sample\")\n",
    "    axs[0].set_title(f\"Imputation Validation Samples for {name}\", fontsize=12)\n",
    "    axs[3].set_yticklabels([])\n",
    "    axs[3].grid(False)\n",
    "    plt.colorbar(missing, cax=cbar_ax,)\n",
    "    cbar_ax.set_yticks([], minor=False, major=False)\n",
    "    cbar_ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    for lab, v in zip(train_n.columns.to_list(), np.linspace(1, 14.2, train_n.shape[1])):\n",
    "        cbar_ax.text(-7, v, lab, fontsize=9)\n",
    "    cbar_ax.set_title(\"Missing Features\")\n",
    "    fig.savefig('figures/qual_{mod}.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation\n",
    "\n",
    "Here we use the trained models on the blind well test sets to see if the predictive accuracy is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_test_dfs = dict()\n",
    "imputation_test_keys = ['DT', 'DTS', 'RHOB']\n",
    "\n",
    "for key in imputation_test_keys:\n",
    "    imputation_test_dfs[key] = data_prep(test.copy(), key, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute test sets\n",
    "imputed_test = dict()\n",
    "for imp_mod, args in imputation_args.items():\n",
    "    for key, val in imputation_test_dfs.items():\n",
    "        temp_df = val[0].copy()\n",
    "        temp_df.loc[:, args[\"training_set\"]] = models[imp_mod+'_'+key].transform(val[0][args[\"training_set\"]])\n",
    "        imputed_test[imp_mod+'_'+key] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_test2 = dict()\n",
    "for imp_mod, args in imputation_args2.items():\n",
    "    for key, val in imputation_test_dfs.items():\n",
    "        temp_df = val[0].copy()\n",
    "        temp_df.loc[:, args[\"training_set\"]] = models2[imp_mod+'_'+key].transform(val[0][args[\"training_set\"]])\n",
    "        imputed_test2[imp_mod+'_'+key] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute tests sets using lgbonce models\n",
    "dt_one_imputed_test = imputation_test_dfs['DT'][0].copy()\n",
    "dts_one_imputed_test = imputation_test_dfs['DTS'][0].copy()\n",
    "rhob_one_imputed_test = imputation_test_dfs['RHOB'][0].copy()\n",
    "dt_one_imputed_test.loc[dt_one_imputed_test[\"DT\"].isna(), \"DT\"] = dt_lgbm.predict(imputation_test_dfs['DT'][0][dt_col])[dt_one_imputed_test[\"DT\"].isna().values]\n",
    "dts_one_imputed_test.loc[dts_one_imputed_test[\"DTS\"].isna(), \"DTS\"] = dts_lgbm.predict(imputation_test_dfs['DTS'][0][dts_col])[dts_one_imputed_test[\"DTS\"].isna().values]\n",
    "rhob_one_imputed_test.loc[rhob_one_imputed_test[\"RHOB\"].isna(), \"RHOB\"] = rho_lgbm.predict(imputation_test_dfs['RHOB'][0][rhob_col])[rhob_one_imputed_test[\"RHOB\"].isna().values]\n",
    "\n",
    "imputed_test[\"lgbonce_DT\"] = dt_one_imputed_test\n",
    "imputed_test[\"lgbonce_DTS\"] = dts_one_imputed_test\n",
    "imputed_test[\"lgbonce_RHOB\"] = rhob_one_imputed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = dict()\n",
    "for key, d in imputed_test.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores_test[f'{key}_{mod}'] = evaluate(d, test, key)\n",
    "pd.DataFrame(scores_test).sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data1, data2, j, no_masking=False):\n",
    "    \"\"\"Evaluate the models against the NANed data from the training set.\n",
    "    \"\"\"\n",
    "    if not no_masking:\n",
    "        mask = data1.set_nan.values\n",
    "    else:\n",
    "        mask = ~np.logical_or(data2[j].isna(), data1[j].isna()).values\n",
    "    truth = data2.loc[mask, j].values\n",
    "    test = data1.loc[mask, j].values\n",
    "\n",
    "    se = np.power((truth - test)/truth, 2)\n",
    "    score = np.nanmean(np.power(se, 0.5))*100.0\n",
    "    er = dict(\n",
    "        perc_error=score,\n",
    "        explained_var=metrics.explained_variance_score(truth, test),\n",
    "        max_error=metrics.max_error(truth, test),\n",
    "        mae=metrics.mean_absolute_error(truth, test),\n",
    "        mse=metrics.mean_squared_error(truth, test),\n",
    "        r2=metrics.r2_score(truth, test),\n",
    "    )\n",
    "    return er\n",
    "\n",
    "scores_test_sub = dict()\n",
    "for key, d in imputed_test.items():\n",
    "    mod, key = key.split('_')\n",
    "#     print(key, d.iloc[25000:31000, :])\n",
    "    scores_test_sub[f'{key}_{mod}'] = evaluate(d.iloc[25000:31000, :], test_noedits.iloc[25000:31000, :], key, no_masking=True)\n",
    "pd.DataFrame(scores_test_sub).sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.iloc[25000:31000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test2 = dict()\n",
    "for key, d in imputed_test2.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores_test2[f'{key}_{mod}'] = evaluate(d, test, key)\n",
    "pd.DataFrame(scores_test2).sort_index(axis=1).T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from matplotlib import gridspec\n",
    "\n",
    "c_map = colors.ListedColormap(['white', 'red', 'blue', 'green'])\n",
    "bounds = [-15, 35, 45, 55, 65]\n",
    "norm = colors.BoundaryNorm(bounds, c_map.N)\n",
    "\n",
    "for mod, name in zip(['lgbrand', 'knn', 'knn1', 'brr', 'brr1', 'lgbonce'], ['GBT Random', 'KNN Regresssion', 'KNN Regresssion Once', 'Bayesian Ridge Regression', 'Bayesian Ridge Regression Once', \"GBT Direct\"]):\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, gridspec_kw={\"width_ratios\":[9, 1]}, figsize=(15, 7), sharex=\"col\")\n",
    "    \n",
    "    gs = axs[2, 1].get_gridspec()\n",
    "    # remove the underlying axes\n",
    "    for ax in axs[:, 1]:\n",
    "        ax.remove()\n",
    "    cbar_ax = fig.add_subplot(gs[2:, -1])\n",
    "    \n",
    "    \n",
    "#     cb = axs[:, 1]\n",
    "    axs = axs[:, 0]\n",
    "    axs[0].plot(imputed_test[f\"{mod}_DT\"].DT.values)\n",
    "    axs[0].plot(test_noedits.DT.values)\n",
    "    axs[0].set_ylabel(\"DT\")\n",
    "    axs[1].plot(imputed_test[f\"{mod}_DTS\"].DTS.values)\n",
    "    axs[1].plot(test_noedits.DTS.values)\n",
    "    axs[1].set_ylabel(\"DTS\")\n",
    "    axs[2].plot(imputed_test[f\"{mod}_RHOB\"].RHOB.values)\n",
    "    axs[2].plot(test_noedits.RHOB.values)\n",
    "    axs[2].set_ylabel(\"RHOB\")\n",
    "\n",
    "    test_n = test/test\n",
    "    for i, col in enumerate(test_n.columns):\n",
    "        test_n[col] = (i + 1)\n",
    "        test_n.loc[~test[col].isna().values, col] = np.nan\n",
    "        \n",
    "    missing = axs[3].imshow(test_n.T.values, aspect='auto', cmap=cm.get_cmap('tab20', test_n.shape[1]), interpolation='none', origin='lower')\n",
    "    axs[3].set_ylabel(\"Missing Features\")\n",
    "    axs[3].set_xlabel(\"Sample\")\n",
    "    axs[0].set_title(f\"Imputation Validation Samples for {name}\", fontsize=12)\n",
    "    axs[3].set_yticklabels([])\n",
    "    axs[3].grid(False)\n",
    "    plt.colorbar(missing, cax=cbar_ax,)\n",
    "    cbar_ax.set_yticks([], minor=False, major=False)\n",
    "    cbar_ax.set_yticklabels([])\n",
    "    \n",
    "    \n",
    "    for lab, v in zip(test_n.columns.to_list(), np.linspace(1, 14.2, test_n.shape[1])):\n",
    "        cbar_ax.text(-7, v, lab, fontsize=9)\n",
    "    cbar_ax.set_title(\"Missing Features\")\n",
    "    fig.savefig(f'figures/qual_test_{mod}.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=9, figsize=(15, 20), sharey=\"row\", sharex=\"col\")\n",
    "mod='lgbasc'\n",
    "logs = [ 'ZONE', 'GR', 'NPHI', 'PEF', 'RM10', 'RD10', 'DT', 'DTS', \"RHOB\"]\n",
    "for row, (well, val) in zip(range(4), train.groupby(\"WELL\")):\n",
    "    axs[row, -3].plot(imputed[f\"{mod}_DT\"].loc[val.index, \"DT\"], -val.TVDSS, 'g')\n",
    "    axs[row, -2].plot(imputed[f\"{mod}_DTS\"].loc[val.index, \"DTS\"], -val.TVDSS, 'g')\n",
    "    axs[row, -1].plot(imputed[f\"{mod}_RHOB\"].loc[val.index, \"RHOB\"], -val.TVDSS, 'g')\n",
    "    for col, log in enumerate(logs):\n",
    "        axs[row, col].plot(val[log], -val.TVDSS, 'k', alpha=0.5)\n",
    "for col, log in enumerate(logs):\n",
    "    axs[0, col].set_title(log)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Well Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(index=df.index)\n",
    "df_pred['dte']= dt_lgbm.predict(df[dt_col])\n",
    "df_pred['dtse'] = dts_lgbm.predict(df[dts_col])\n",
    "df_pred['rhob'] = rho_lgbm.predict(df[rhob_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in data.Well.unique():\n",
    "    fig, axs = plt.subplots(ncols=9, figsize=(8, 8), sharey=True)\n",
    "\n",
    "    well = data.query(\"Well == @w\")\n",
    "    sub = df.loc[well.index, :]\n",
    "    sub_pred = df_pred.loc[well.index, :]\n",
    "\n",
    "    frame = 0\n",
    "    axs[frame].plot(sub_pred.dte, -well.TVDSS, color='green')\n",
    "    axs[frame].plot(sub.DTE,  -well.TVDSS, color='#4d4f4e')\n",
    "    axs[frame].set_title('DT')\n",
    "\n",
    "    frame+=1\n",
    "    axs[frame].plot(sub_pred.dtse, -well.TVDSS, color='green')\n",
    "    axs[frame].plot(sub.DTSE,  -well.TVDSS, color='#4d4f4e')\n",
    "\n",
    "    axs[frame].set_title('DTS')\n",
    "\n",
    "    frame+=1\n",
    "    axs[frame].plot(sub_pred.rhob, -well.TVDSS, color='green')\n",
    "    axs[frame].plot(sub.RHOBE,  -well.TVDSS, color='#4d4f4e')\n",
    "    axs[frame].set_title('RHOB')\n",
    "\n",
    "    for name, idx in (\n",
    "        ('GR', 'GRE'), ('NPHI', 'NPHIE'), ('PEF', 'PEFE'), ('log(Rd)', 'RDElog'), ('log(Rm)', 'RMElog'), ('Zone', 'ZONE_NO')\n",
    "    ):\n",
    "        frame+=1\n",
    "        axs[frame].plot(sub[idx], -well.TVDSS, color='#4d4f4e')\n",
    "        axs[frame].set_title(name)\n",
    "        \n",
    "    w = encoder.inverse_transform([w])[0]\n",
    "    fig = plt.gcf()\n",
    "    rect = fig.patch\n",
    "    rect.set_facecolor('white')\n",
    "    fig.tight_layout(pad=0.5, w_pad=0, h_pad=0.0)\n",
    "    fig.savefig(f'figures/imputation/lgbm_{w}_logs_pred.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, figsize=(20, 12))\n",
    "\n",
    "mask = imputed['lgbrand_DTE'].set_nan\n",
    "axs[0].plot(df[mask].DTE.values)\n",
    "axs[0].plot(dt_pred1[mask])\n",
    "axs[0].plot(imputed['lgbrand_DTE'][mask].DTE.values)\n",
    "\n",
    "# err1 = np.power(np.power((df.loc[mask, 'DTE'].values - dt_pred1[mask])/df.loc[mask, 'DTE'], 2), 0.5)[mask].values\n",
    "# err2 = np.power(np.power((df.loc[mask, 'DTE'].values - imputed['lgbrand_DTE'][mask].DTE.values)/df.loc[mask, 'DTE'], 2), 0.5)[mask].values\n",
    "# axs[1].plot(err1)\n",
    "# axs[1].plot(err2)\n",
    "\n",
    "mask = imputed['lgbrand_DTSE'].set_nan\n",
    "axs[1].plot(df[mask].DTSE.values)\n",
    "axs[1].plot(dts_pred1[mask])\n",
    "axs[1].plot(imputed['lgbrand_DTSE'][mask].DTSE.values)\n",
    "\n",
    "mask = imputed['lgbrand_RHOBE'].set_nan\n",
    "axs[2].plot(df[mask].RHOBE.values)\n",
    "axs[2].plot(rho_pred1[mask])\n",
    "axs[2].plot(imputed['lgbrand_RHOBE'][mask].RHOBE.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed['lgbrand_DTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def evaluate(data1, data2, j):\n",
    "    se = np.power((data1.loc[data1.set_nan.values, j] - data2.loc[data1.set_nan.values, j])/data2.loc[data1.set_nan.values, j], 2)\n",
    "    score = np.nanmean(np.power(se, 0.5))*100.0\n",
    "    #score = mean_squared_error(data2.loc[data1.set_nan, j].values, data1.loc[data1.set_nan, j].values)\n",
    "    return score\n",
    "\n",
    "scores = defaultdict(dict)\n",
    "for key, d in imputed_for.items():\n",
    "    mod, key = key.split('_')\n",
    "    scores[mod][key] = evaluate(d, df, key)\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Cross Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(10,7.5))\n",
    "\n",
    "for mod, axrow, name in zip(['lgbrand', 'knn', 'brr'], axs, ['LightGBM', 'KNN Regresssion', 'Bayesian Ridge Regression']):\n",
    "    for key, ax in zip(impute_for.keys(), axrow):\n",
    "        model = imputed[mod+'_'+key]\n",
    "        set_nan = model.set_nan\n",
    "        smin = np.nanmin(df.loc[set_nan, key].values)\n",
    "        smax = np.nanmax(df.loc[set_nan, key].values)\n",
    "        ax.scatter(df.loc[set_nan, key].values, model.loc[set_nan, key].values, c=model.loc[set_nan, 'ZONE_NO'].values, alpha=0.1, cmap='Set3', marker='.')\n",
    "        ax.set_xlim(smin, smax)\n",
    "        ax.set_ylim(smin, smax)\n",
    "        ax.plot((smin, smax), (smin, smax), '--', color='r')\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(key[:-1])\n",
    "\n",
    "    axrow[0].set_ylabel(f'Imputed {name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('figures/imputation/versus.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainers = [shap.TreeExplainer(model.estimator, feature_perturbation=\"tree_path_dependent\") for model in models_gbt['DTE'].imputation_sequence_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models_gbt['DTE'].imputation_sequence_[9:18] + models_gbt['DTE'].imputation_sequence_[-9:]:\n",
    "    print(m.neighbor_feat_idx, m.feat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vals_end = []\n",
    "for exp, mod in zip(explainers[-9:], models_gbt['DTE'].imputation_sequence_[-9:]):\n",
    "    shap_vals_end.append(exp.shap_values(impute_for['DTE'][0].iloc[:, mod.neighbor_feat_idx]) \n",
    "    )\n",
    "shap_vals_beg = []\n",
    "for exp, mod in zip(explainers[9:18], models_gbt['DTE'].imputation_sequence_[9:18]):\n",
    "    shap_vals_beg.append(exp.shap_values(impute_for['DTE'][0].iloc[:, mod.neighbor_feat_idx]) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "sel = np.random.randint(0, df.index.size, size=df.index.size) <= 500\n",
    "sub = df.iloc[sel, :]\n",
    "sub = sub.loc[:, ['DTSE', 'GRE', 'NPHIE', 'PEFE', 'RHOBE', 'RMElog', 'RDElog', 'ZONE_NO']]\n",
    "shap.force_plot(exp.expected_value, shap_vals[0][sel,:], sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename_map = {\n",
    "    'ZONE_NO':'ZONE',\n",
    "    'DTE':'DT',\n",
    "    'DTSE':'DTS',\n",
    "    'GRE':'GR',\n",
    "    'NPHIE':'NPHI',\n",
    "    'PEFE':'PEF',\n",
    "    'RHOBE':'RHOB',\n",
    "    'RMElog':'LogRM',\n",
    "    'RDElog':'LogRD'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['DTE', 'DTSE', 'GRE', 'NPHIE', 'PEFE', 'RHOBE', 'RMElog', 'RDElog', 'ZONE_NO']\n",
    "sel = np.random.randint(0, df.index.size, size=df.index.size) <= 100\n",
    "sub = df.iloc[sel, :]\n",
    "for i, m in enumerate(range(-9, 0, 1)):\n",
    "    mod = models_gbt['DTE'].imputation_sequence_[m]\n",
    "    m_names = [names[j] for j in mod.neighbor_feat_idx]\n",
    "    shap.summary_plot(\n",
    "        shap_vals_end[i],\n",
    "        df[m_names].rename(columns=col_rename_map),\n",
    "        title=f\"{names[mod.feat_idx]}\",\n",
    "        color_bar_label=f\"Feature Value {names[mod.feat_idx]}\",\n",
    "        show=False,\n",
    "        plot_size=(5, 3),\n",
    "    )\n",
    "    fig = plt.gcf()\n",
    "    rect = fig.patch\n",
    "    rect.set_facecolor('white')\n",
    "    fig.tight_layout(pad=0.5, w_pad=0, h_pad=0.0)\n",
    "    fig.savefig(f'figures/imputation/{names[mod.feat_idx]}_end.png', dpi=150)\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'ZONE_NO':'ZONE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['DTE', 'DTSE', 'GRE', 'NPHIE', 'PEFE', 'RHOBE', 'RMElog', 'RDElog', 'ZONE_NO']\n",
    "sel = np.random.randint(0, df.index.size, size=df.index.size) <= 100\n",
    "sub = df.iloc[sel, :]\n",
    "for i, m in enumerate(range(9, 18, 1)):\n",
    "    mod = models_gbt['DTE'].imputation_sequence_[m]\n",
    "    m_names = [names[j] for j in mod.neighbor_feat_idx]\n",
    "    shap.summary_plot(\n",
    "        shap_vals_beg[i],\n",
    "        df[m_names].rename(columns=col_rename_map),\n",
    "        title=f\"{names[mod.feat_idx]}\",\n",
    "        color_bar_label=f\"Feature Value {names[mod.feat_idx]}\",\n",
    "        show=False,\n",
    "        plot_size=(5, 3)\n",
    "    )\n",
    "    fig = plt.gcf()\n",
    "    rect = fig.patch\n",
    "    rect.set_facecolor('white')\n",
    "    fig.tight_layout(pad=0.5, w_pad=0, h_pad=0.0)\n",
    "    fig.savefig(f'figures/imputation/{names[mod.feat_idx]}_beg.png', dpi=150)\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['DTE', 'DTSE', 'GRE', 'NPHIE', 'PEFE', 'RHOBE', 'RMElog', 'RDElog', 'ZONE_NO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation Error\n",
    "\n",
    "In this section we explore the impact of data sparsity on the predictions. The sparsity is introduced through random removal of data from model in each column. The SHAP analysis should answer other questions about feature importance e.g. missing features and how that might impact prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_all(data, j, set_to_nan=0.3):\n",
    "    data = data.copy()\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if col == 'j':\n",
    "            continue\n",
    "        rand_set_mask = np.random.random(data.shape[0]) < set_to_nan\n",
    "        data.loc[data.index[rand_set_mask], col] = np.nan\n",
    "    \n",
    "    sub = data.dropna(subset=[j])\n",
    "    rand_set_mask = np.random.random(len(sub)) < set_to_nan\n",
    "    replace = sub.index[rand_set_mask]\n",
    "    data.loc[replace, j] = np.nan\n",
    "    data['set_nan'] = False\n",
    "    data.loc[replace, 'set_nan'] = True\n",
    "    data['was_nan'] = data[j].isna()\n",
    "    return data\n",
    "\n",
    "def evaluate(data1, data2, j):\n",
    "    mask = data1.set_nan.values\n",
    "    truth = data2.loc[mask, j].values\n",
    "    test = data1.loc[mask, j].values\n",
    "    se = np.power((truth - test)/truth, 2)\n",
    "    score = np.nanmean(np.power(se, 0.5))*100.0\n",
    "    er = dict(\n",
    "        perc_error=score,\n",
    "        explained_var=metrics.explained_variance_score(truth, test),\n",
    "        max_error=metrics.max_error(truth, test),\n",
    "        mae=metrics.mean_absolute_error(truth, test),\n",
    "        mse=metrics.mean_squared_error(truth, test),\n",
    "        r2=metrics.r2_score(truth, test),\n",
    "    )\n",
    "    return er\n",
    "\n",
    "blank_scores = dict(\n",
    "                    perc_error=np.nan,\n",
    "                    explained_var=np.nan,\n",
    "                    max_error=np.nan,\n",
    "                    mae=np.nan,\n",
    "                    mse=np.nan,\n",
    "                    r2=np.nan,\n",
    "                )\n",
    "\n",
    "\n",
    "# models_err = dict()\n",
    "# imputed_err = defaultdict(dict)\n",
    "# impute_for_err = dict()\n",
    "# for key in ['DT', 'DTS', 'RHOB']:\n",
    "#     for missing in np.linspace(0.1, 0.9, 9):\n",
    "#         mkey = f'{missing:0.1f}'\n",
    "#         temp_df = train.copy()\n",
    "#         temp_df.fillna(0.0)\n",
    "#         impute_for_err[key+'_'+mkey] = data_prep_all(temp_df.copy(), key, missing)\n",
    "    \n",
    "for key in ['DT', 'DTS', 'RHOB']:\n",
    "    for missing in np.linspace(0.1, 0.9, 9):\n",
    "        mkey = f'{missing:0.1f}'\n",
    "#         for imp_mod, args in imputation_args.items():\n",
    "        for imp_mod, args in [(key, imputation_args[key]) for key in [\"brr1\", \"knn1\"]]:\n",
    "\n",
    "            val = impute_for_err[f'{key}_{mkey}']\n",
    "            try:\n",
    "                models_err[f'{imp_mod}_{key}_{mkey}'] = train_models(val.copy(), **args)\n",
    "\n",
    "                scores = evaluate(models_err[f'{imp_mod}_{key}_{mkey}'][0], train, key)\n",
    "            except:\n",
    "                scores = blank_scores\n",
    "            scores['fmissing'] = missing\n",
    "            scores['key'] = key\n",
    "            scores['model'] = imp_mod\n",
    "            imputed_err[f'{imp_mod}_{key}_{mkey}'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for missing in np.linspace(0.1, 0.9, 9):\n",
    "    mkey = f'{missing:0.1f}'\n",
    "    dt_col = ['DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "    dts_col = ['DT', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "    rhob_col = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RM10', 'ZONE',]\n",
    "\n",
    "    # DT\n",
    "    try:\n",
    "        temp_pred = imputation_test_dfs[\"DT\"][0].copy()\n",
    "        temp_pred['DT'] = dt_lgbm.predict(val.copy().loc[:, dt_col])\n",
    "        scores = evaluate(temp_pred, train, 'DT')\n",
    "    except:\n",
    "        scores = blank_scores\n",
    "    scores['fmissing'] = missing\n",
    "    scores['key'] = 'DT'\n",
    "    scores['model'] = 'lgbm1'\n",
    "    imputed_err_test[f'lgb1_DT_{mkey}'] = scores\n",
    "    \n",
    "    # DTS\n",
    "    try:\n",
    "        temp_pred = imputation_test_dfs[\"DTS\"][0].copy()\n",
    "        temp_pred['DTS'] = dts_lgbm.predict(val.copy().loc[:, dts_col])\n",
    "        scores = evaluate(temp_pred, train, 'DTS')\n",
    "    except:\n",
    "        scores = blank_scores\n",
    "    scores['fmissing'] = missing\n",
    "    scores['key'] = 'DTS'\n",
    "    scores['model'] = 'lgbm1'\n",
    "    imputed_err_test[f'lgb1_DTS_{mkey}'] = scores\n",
    "\n",
    "    # RHOB\n",
    "    try:\n",
    "        temp_pred = imputation_test_dfs[\"RHOB\"][0].copy()\n",
    "        temp_pred['RHOB'] = rho_lgbm.predict(val.copy().loc[:, rhob_col])\n",
    "        scores = evaluate(temp_pred, train, 'RHOB')\n",
    "    except:\n",
    "        scores=blank_scores\n",
    "    scores['fmissing'] = missing\n",
    "    scores['key'] = 'RHOB'\n",
    "    scores['model'] = 'lgbm1'\n",
    "    imputed_err_test[f'lgb1_RHOB_{mkey}'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_err_test = dict()\n",
    "for key, (_, imputer) in models_err.items():\n",
    "    modk, log, fmissing = key.split(\"_\")\n",
    "    training_set = imputation_args[modk][\"training_set\"]\n",
    "    temp_df = imputation_test_dfs[log][0].copy()\n",
    "    temp_df.loc[:, training_set] = imputer.transform(temp_df[training_set])\n",
    "    scores = evaluate(temp_df, test, log)\n",
    "    \n",
    "    scores['fmissing'] = fmissing\n",
    "    scores['key'] = log\n",
    "    scores['model'] = modk\n",
    "    imputed_err_test[key] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run error testing for single pass\n",
    "\n",
    "for missing in np.linspace(0.1, 0.9, 9):\n",
    "    mkey = f'{missing:0.1f}'\n",
    "    dt_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100, n_jobs=4)\n",
    "    dts_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100, n_jobs=4)\n",
    "    rho_lgbm = LGBMRegressor(random_state=456, max_depth=5, num_leaves=100, n_jobs=4)\n",
    "    dt_col = ['DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "    dts_col = ['DT', 'GR', 'NPHI', 'PEF', 'RD10', 'RHOB', 'RM10', 'ZONE',]\n",
    "    rhob_col = ['DT', 'DTS', 'GR', 'NPHI', 'PEF', 'RD10', 'RM10', 'ZONE',]\n",
    "\n",
    "    # DT\n",
    "    val = impute_for_err[f'DT_{mkey}']\n",
    "\n",
    "    try:\n",
    "        dt_lgbm.fit(\n",
    "            val.copy().dropna(subset=['DT']).loc[:, dt_col], \n",
    "            val.copy().dropna(subset=['DT'])['DT']\n",
    "        )\n",
    "\n",
    "        temp_pred = val.copy()\n",
    "        temp_pred['DT'] = dt_lgbm.predict(val.copy().loc[:, dt_col])\n",
    "        temp_pred_test = imputation_test_dfs[\"DT\"][0].copy()\n",
    "        temp_pred_test['DT'] = dt_lgbm.predict(temp_pred_test.loc[:, dt_col])\n",
    "        scores = evaluate(temp_pred, train, 'DT')\n",
    "        scores_test = evaluate(temp_pred_test, test, \"DT\")\n",
    "    except:\n",
    "        scores = blank_scores\n",
    "        scores_test = blank_scores\n",
    "        \n",
    "    scores.update(dict(fmissing=mkey, key=\"DT\", model=\"lgbm1\"))\n",
    "    imputed_err[f'lgb1_DT_{mkey}'] = scores\n",
    "    scores_test.update(dict(fmissing=mkey, key=\"DT\", model=\"lgbm1\"))\n",
    "    imputed_err_test[f'lgb1_DT_{mkey}'] = scores_test\n",
    "    \n",
    "    # DTS\n",
    "    val = impute_for_err[f'DTS_{mkey}']\n",
    "\n",
    "    try:\n",
    "        dts_lgbm.fit(\n",
    "            val.copy().dropna(subset=['DTS']).loc[:, dts_col], \n",
    "            val.copy().dropna(subset=['DTS'])['DTS']\n",
    "        )\n",
    "\n",
    "        temp_pred = val.copy()\n",
    "        temp_pred['DTS'] = dts_lgbm.predict(val.copy().loc[:, dts_col])\n",
    "        scores = evaluate(temp_pred, train, 'DTS')\n",
    "        temp_pred_test = imputation_test_dfs[\"DTS\"][0].copy()\n",
    "        temp_pred_test['DTS'] = dts_lgbm.predict(temp_pred_test.loc[:, dts_col])\n",
    "        scores = evaluate(temp_pred, train, 'DTS')\n",
    "        scores_test = evaluate(temp_pred_test, test, \"DTS\")\n",
    "    except:\n",
    "        scores = blank_scores\n",
    "        scores_test = blank_scores\n",
    "        \n",
    "    scores.update(dict(fmissing=mkey, key=\"DTS\", model=\"lgbm1\"))\n",
    "    imputed_err[f'lgb1_DTS_{mkey}'] = scores\n",
    "    scores_test.update(dict(fmissing=mkey, key=\"DTS\", model=\"lgbm1\"))\n",
    "    imputed_err_test[f'lgb1_DTS_{mkey}'] = scores_test\n",
    "\n",
    "    # RHOB\n",
    "    val = impute_for_err[f'RHOB_{mkey}']\n",
    "\n",
    "    try:\n",
    "        rho_lgbm.fit(\n",
    "            val.copy().dropna(subset=['RHOB']).loc[:, rhob_col], \n",
    "            val.copy().dropna(subset=['RHOB'])['RHOB']\n",
    "        )\n",
    "\n",
    "        temp_pred = val.copy()\n",
    "        temp_pred['RHOB'] = rho_lgbm.predict(val.copy().loc[:, rhob_col])\n",
    "        scores = evaluate(temp_pred, train, 'RHOB')\n",
    "        temp_pred_test = imputation_test_dfs[\"RHOB\"][0].copy()\n",
    "        temp_pred_test['RHOB'] = rho_lgbm.predict(temp_pred_test.loc[:, rhob_col])\n",
    "        scores = evaluate(temp_pred, train, 'RHOB')\n",
    "        scores_test = evaluate(temp_pred_test, test, \"RHOB\")\n",
    "    except:\n",
    "        scores = blank_scores\n",
    "        scores_test = blank_scores\n",
    "\n",
    "    scores.update(dict(fmissing=mkey, key=\"RHOB\", model=\"lgbm1\"))\n",
    "    imputed_err[f'lgb1_RHOB_{mkey}'] = scores\n",
    "    scores_test.update(dict(fmissing=mkey, key=\"RHOB\", model=\"lgbm1\"))\n",
    "    print(scores_test)\n",
    "    imputed_err_test[f'lgb1_RHOB_{mkey}'] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(imputed_err).T)\n",
    "err_results_df = pd.DataFrame(imputed_err).T\n",
    "err_results_df.to_csv('missing_err.csv', index=True)\n",
    "imputed_err_test_df = pd.DataFrame(imputed_err_test).T\n",
    "\n",
    "for col in [\"perc_error\", \"explained_var\", \"max_error\", \"mae\", \"mse\", \"r2\", \"fmissing\"]:\n",
    "    err_results_df[col] = pd.to_numeric(err_results_df[col])\n",
    "    imputed_err_test_df[col] = pd.to_numeric(imputed_err_test_df[col])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(imputed_err, open( \"imputed_err.p\", \"wb\" ))\n",
    "pickle.dump(models_err, open(\"models_err.p\", \"wb\"))\n",
    "\n",
    "# imputed_err = pickle.load(open(\"imputed_err.p\", \"rb\"))\n",
    "# models_err = pickle.load(open(\"models_err.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imputed_err\n",
    "del models_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, mt, ys in zip(\n",
    "    ['perc_error', 'explained_var', 'max_error', 'mae', 'mse', 'r2'],\n",
    "    ['Percentage Imputation Error', 'Explained Variance', 'Maximum Error', 'Mean Absolute Error', 'Mean Squared Error', r'$R^2$'],\n",
    "    [True, True, False, False, False, True]\n",
    "):\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(15, 4), sharex=True, sharey=ys)\n",
    "\n",
    "    sns.set_context('paper')\n",
    "\n",
    "    for log, ax in zip(['DT', 'DTS', 'RHOB'], axs):\n",
    "        sub = err_results_df.query(\"key == @log\")\n",
    "        for mod, name in zip(['lgbrand', 'brr', 'brr1', 'knn', 'knn1', 'lgbm1'], ['LightGBM', 'BRR', 'SingleBRR', 'KNN', 'SingleKNN', 'SingleLGBM']):\n",
    "            sub_plot = sub.query(\"model == @mod\").sort_values(\"fmissing\")\n",
    "            ax.plot(sub_plot.fmissing, sub_plot[measure], label=name)\n",
    "        ax.set_title(log)\n",
    "        ax.set_xlabel('Fraction of Data Missing')\n",
    "        ax.set_ylabel(mt)\n",
    "        ax.legend()\n",
    "    fig.savefig(f'figures/error_{measure}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, mt, ys in zip(\n",
    "    ['perc_error', 'explained_var', 'max_error', 'mae', 'mse', 'r2'],\n",
    "    ['Percentage Imputation Error', 'Explained Variance', 'Maximum Error', 'Mean Absolute Error', 'Mean Squared Error', r'$R^2$'],\n",
    "    [True, True, False, False, False, True]\n",
    "):\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(15, 4), sharex=True, sharey=ys)\n",
    "\n",
    "    sns.set_context('paper')\n",
    "\n",
    "    for log, ax in zip(['DT', 'DTS', 'RHOB'], axs):\n",
    "        sub = imputed_err_test_df.query(\"key == @log\")\n",
    "        for mod, name in zip(['lgbrand', 'brr', 'brr1', 'knn', 'knn1', 'lgbm1'], ['LightGBM', 'BRR', 'SingleBRR', 'KNN', 'SingleKNN', 'SingleLGBM']):\n",
    "            sub_plot = sub.query(\"model == @mod\").sort_values(\"fmissing\")\n",
    "            ax.plot(sub_plot.fmissing.values, sub_plot[measure].values, label=name)\n",
    "        ax.set_title(log)\n",
    "        ax.set_xlabel('Fraction of Data Missing')\n",
    "        ax.set_ylabel(mt)\n",
    "        ax.set_ylim(0)\n",
    "        ax.legend()\n",
    "    fig.savefig(f'figures/error_test_{measure}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Actual Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_index = data.query(\"WELL == 'F-12'\")\n",
    "# well_index = data.query(\"WELL == 'F-4'\")\n",
    "\n",
    "\n",
    "#apply model\n",
    "def apply_model(df, models):\n",
    "    apply_set = ['DTE', 'DTSE', 'GRE', 'NPHIE', 'PEFE', 'RHOBE', 'RMElog', 'RDElog', 'ZONE_NO']\n",
    "    output = df.copy()\n",
    "    for key, mod in models.items():\n",
    "        index = apply_set.index(key)\n",
    "        filled = mod.transform(df[apply_set])\n",
    "        output.loc[:, key] = filled[:, index]\n",
    "    return output\n",
    "\n",
    "filled = apply_model(df.loc[well_index.index, :], models_gbt)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(20,10))\n",
    "for key, ax in zip(impute_for.keys(), axs):\n",
    "    ax.plot(filled[key].values)\n",
    "    ax.plot(df.loc[well_index.index, key].values)\n",
    "#     ax.plot(imputed_brr[key].loc[set_nan, key].values, color='r')\n",
    "    ax.set_title(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "missing value prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-logs",
   "language": "python",
   "name": "ml-logs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
